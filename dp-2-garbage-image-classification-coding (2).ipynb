{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7973614,"sourceType":"datasetVersion","datasetId":4692121}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Activation, Add, Dense, Conv2D, GlobalAveragePooling2D, MaxPooling2D\nfrom keras.layers import BatchNormalization, Dropout\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:49:35.834615Z","iopub.execute_input":"2024-03-31T15:49:35.835592Z","iopub.status.idle":"2024-03-31T15:49:49.430857Z","shell.execute_reply.started":"2024-03-31T15:49:35.835544Z","shell.execute_reply":"2024-03-31T15:49:49.430059Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-03-31 15:49:39.633926: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-31 15:49:39.634051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-31 15:49:39.762281: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_path='/kaggle/input/dataset/Garbage classification'\ngarbage_types = os.listdir(dataset_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:50:03.821869Z","iopub.execute_input":"2024-03-31T15:50:03.822476Z","iopub.status.idle":"2024-03-31T15:50:03.827642Z","shell.execute_reply.started":"2024-03-31T15:50:03.822443Z","shell.execute_reply":"2024-03-31T15:50:03.826538Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Here I'm going to create the dataframe for thr image ---> Basically, a image pipeline\ndata = []\nfor garbage_type in garbage_types:\n    for file in os.listdir(os.path.join(dataset_path, garbage_type)):\n        data.append((os.path.join(dataset_path, garbage_type, file), garbage_type))\ndf = pd.DataFrame(data, columns=['filepath', 'label'])\ndf.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-31T15:50:10.290097Z","iopub.execute_input":"2024-03-31T15:50:10.290850Z","iopub.status.idle":"2024-03-31T15:50:11.303772Z","shell.execute_reply.started":"2024-03-31T15:50:10.290818Z","shell.execute_reply":"2024-03-31T15:50:11.302758Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                            filepath  label\n0  /kaggle/input/dataset/Garbage classification/m...  metal\n1  /kaggle/input/dataset/Garbage classification/m...  metal\n2  /kaggle/input/dataset/Garbage classification/m...  metal\n3  /kaggle/input/dataset/Garbage classification/m...  metal\n4  /kaggle/input/dataset/Garbage classification/m...  metal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/dataset/Garbage classification/m...</td>\n      <td>metal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/dataset/Garbage classification/m...</td>\n      <td>metal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/dataset/Garbage classification/m...</td>\n      <td>metal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/dataset/Garbage classification/m...</td>\n      <td>metal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/dataset/Garbage classification/m...</td>\n      <td>metal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# SPlit into different train and test dataset\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# Print the number of images in each set\nprint(f\"Number of images in the training set: {len(train_df)}\")\nprint(f\"Number of images in the validation set: {len(val_df)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:50:20.122455Z","iopub.execute_input":"2024-03-31T15:50:20.122816Z","iopub.status.idle":"2024-03-31T15:50:20.140611Z","shell.execute_reply.started":"2024-03-31T15:50:20.122787Z","shell.execute_reply":"2024-03-31T15:50:20.139700Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Number of images in the training set: 2021\nNumber of images in the validation set: 506\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here I will Perfome the Data Augumentation to increase the size of the daata","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1./255,                     \n    rotation_range=45,                 \n    width_shift_range=0.15,             \n    height_shift_range=0.15,            \n    zoom_range=0.15,                   \n    horizontal_flip=True,             \n    vertical_flip=True,                 \n    shear_range=0.05,                   \n    brightness_range=[0.9, 1.1],       \n    channel_shift_range=10,             \n    fill_mode='nearest'   \n)    \n                   \nval_datagen =ImageDataGenerator(rescale=1./255)   ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:50:25.042483Z","iopub.execute_input":"2024-03-31T15:50:25.043228Z","iopub.status.idle":"2024-03-31T15:50:25.048669Z","shell.execute_reply.started":"2024-03-31T15:50:25.043194Z","shell.execute_reply":"2024-03-31T15:50:25.047645Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Here I will create the Batches of the Training Dataset and Testing Dataset because the Image Pipeline is big and Batch Processing will help","metadata":{}},{"cell_type":"code","source":"# For Training Data\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,                  \n    x_col=\"filepath\",                    \n    y_col=\"label\",                        \n    target_size=(384, 384),              \n    batch_size=32,                       \n    class_mode='categorical',            \n    seed=42,                             \n    shuffle=False                       \n)\n\n# For Testing Data\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=val_df,                    \n    x_col=\"filepath\",                    \n    y_col=\"label\",                       # Column with image labels\n    target_size=(384, 384),              # Resize all images to size of 384x384\n    batch_size=32,                       # Number of images per batch\n    class_mode='categorical',            # One-hot encode labels\n    seed=42,                             # Seed for random number generator to ensure reproducibility\n    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n)\n\nlen(train_generator)\nlen(val_generator)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:50:31.549102Z","iopub.execute_input":"2024-03-31T15:50:31.549759Z","iopub.status.idle":"2024-03-31T15:50:33.047491Z","shell.execute_reply.started":"2024-03-31T15:50:31.549722Z","shell.execute_reply":"2024-03-31T15:50:33.046477Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Found 2021 validated image filenames belonging to 6 classes.\nFound 506 validated image filenames belonging to 6 classes.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"16"},"metadata":{}}]},{"cell_type":"code","source":"# CBAM Definition\nimport tensorflow as tf\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Multiply, Conv2D, Add, Activation, BatchNormalization\n\nclass ChannelAttention(tf.keras.layers.Layer):\n    def __init__(self, ratio=8):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = GlobalAveragePooling2D()\n        self.max_pool = GlobalMaxPooling2D()\n        self.shared_mlp = Dense(units=1, activation='relu', use_bias=True, kernel_initializer='he_normal')\n        self.ratio = ratio\n\n    def call(self, x):\n        avg_pool = self.avg_pool(x)\n        max_pool = self.max_pool(x)\n        channel_attentions = self.shared_mlp(tf.concat([avg_pool, max_pool], axis=1))\n        return Multiply()([x, Activation('sigmoid')(channel_attentions)])\n\nclass SpatialAttention(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = Conv2D(filters=1, kernel_size=kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal')\n\n    def call(self, x):\n        avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True)\n        max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n        combined = tf.concat([avg_pool, max_pool], axis=-1)\n        return Multiply()([x, self.conv(combined)])\n\nclass CBAM(tf.keras.layers.Layer):\n    def __init__(self, ratio=8, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttention(ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n\n    def call(self, x):\n        x_out = self.channel_attention(x)\n        return self.spatial_attention(x_out)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:50:50.164159Z","iopub.execute_input":"2024-03-31T15:50:50.165003Z","iopub.status.idle":"2024-03-31T15:50:50.178508Z","shell.execute_reply.started":"2024-03-31T15:50:50.164972Z","shell.execute_reply":"2024-03-31T15:50:50.177602Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Now I will create the model the plain model of ResNet-50 and will further modify it according to the requirement","metadata":{}},{"cell_type":"code","source":"def residual_block(X, kernel_size, filters, reduce=False, stride=2):\n    \"\"\"\n    Implement a residual block for ResNet architectures.\n    \n    Arguments:\n    X           -- input tensor of shape (m, height, width, channels)\n    kernel_size -- integer, kernel size of the middle convolutional layer in the main path\n    filters     -- python list of integers, defining the number of filters in the CONV layers of the main path\n    reduce      -- boolean, whether to reduce the spatial dimensions and increase depth; \n                    if True, applies 1x1 CONV layer to the shortcut path.\n    stride      -- integer, strides for the convolutional layer\n    \n    Returns:\n    X           -- output of the identity block, tensor of shape (height, width, channels)\n    \"\"\"\n     # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. We will need this later to add back to the main path. \n    X_shortcut = X\n    \n    if reduce:\n        # if we are to reduce the spatial size, apply a 1x1 CONV layer to the shortcut path\n        #First Component of main path\n        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (stride,stride), padding = 'valid', kernel_initializer='he_normal')(X)\n        X = BatchNormalization(axis = 3)(X)\n        X = Activation('relu')(X)\n         \n        X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (stride,stride), padding = 'valid', kernel_initializer='he_normal')(X_shortcut)\n        X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n    else: \n        # First component of main path\n        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal')(X)\n        X = BatchNormalization(axis = 3)(X)\n        X = Activation('relu')(X)\n        \n        \n        # Second component of main path\n    X = Conv2D(filters = F2, kernel_size = (kernel_size, kernel_size), strides = (1,1), padding = 'same', kernel_initializer='he_normal')(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n     # Third component of main path\n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal')(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = CBAM()(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a ReLU activation \n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n\n    \n    return X\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T16:53:41.117524Z","iopub.execute_input":"2024-03-31T16:53:41.118216Z","iopub.status.idle":"2024-03-31T16:53:41.129968Z","shell.execute_reply.started":"2024-03-31T16:53:41.118181Z","shell.execute_reply":"2024-03-31T16:53:41.128924Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Original ResNet-50 Definition\n# def ResNet_50(input_shape, classes):\n    \n    \n\n#     # Define the input as a tensor with shape input_shape\n#     X_input = Input(input_shape)\n#      # Block 1\n#     X = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer='he_normal')(X_input)\n#     X = BatchNormalization(axis=3)(X)\n#     X = Activation('relu')(X)\n#     X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n    \n\n    # Block 2\n#     X = residual_block(X, 3, [64, 64, 256], reduce=True, stride=1)\n#     X = residual_block(X, 3, [64, 64, 256])\n#     X = residual_block(X, 3, [64, 64, 256])\n    # Block 3 \n#     X = residual_block(X, 3, [128, 128, 512], reduce=True, stride=2)\n#     X = residual_block(X, 3, [128, 128, 512])\n#     X = residual_block(X, 3, [128, 128, 512])\n#     X = residual_block(X, 3, [128, 128, 512])\n\n    # Block 4 \n#     X = residual_block(X, 3, [256, 256, 1024], reduce=True, stride=2)\n#     X = residual_block(X, 3, [256, 256, 1024])\n#     X = residual_block(X, 3, [256, 256, 1024])\n#     X = residual_block(X, 3, [256, 256, 1024])\n#     X = residual_block(X, 3, [256, 256, 1024])\n#     X = residual_block(X, 3, [256, 256, 1024])\n     # Block 5 \n#     X = residual_block(X, 3, [512, 512, 2048], reduce=True, stride=2)\n#     X = residual_block(X, 3, [512, 512, 2048])\n#     X = residual_block(X, 3, [512, 512, 2048])\n\n    # Global Average Pooling to reduce spatial dimensions\n#     X = GlobalAveragePooling2D()(X)\n    \n    # Fully Connected Layer for classification\n#     X = Dense(classes, activation='softmax')(X)\n        \n    # Create the model\n#     model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n#     return model","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:37.511220Z","iopub.execute_input":"2024-03-31T11:29:37.511657Z","iopub.status.idle":"2024-03-31T11:29:37.524284Z","shell.execute_reply.started":"2024-03-31T11:29:37.511627Z","shell.execute_reply":"2024-03-31T11:29:37.523537Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def Modified_ResNet50(input_shape, classes):\n    \"\"\"\n    Arguments:\n    input_shape -- tuple shape of the images of the dataset\n    classes -- integer, number of classes\n\n    Returns:\n    model -- a Model() instance in Keras\n    \"\"\"\n\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n      # Stage 1\n    X = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer='he_normal')(X_input)\n    X = BatchNormalization(axis=3)(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = residual_block(X, 3, [64, 64, 256], reduce=True, stride=1)\n    X = residual_block(X, 3, [64, 64, 256])\n    X = residual_block(X, 3, [64, 64, 256])\n\n    # Stage 3 \n    X = residual_block(X, 3, [128, 128, 512], reduce=True, stride=2)\n    X = residual_block(X, 3, [128, 128, 512])\n    X = residual_block(X, 3, [128, 128, 512])\n    X = residual_block(X, 3, [128, 128, 512])\n      # Stage 4 \n    X = residual_block(X, 3, [256, 256, 1024], reduce=True, stride=2)\n    X = residual_block(X, 3, [256, 256, 1024])\n    X = residual_block(X, 3, [256, 256, 1024])\n    X = residual_block(X, 3, [256, 256, 1024])\n    X = residual_block(X, 3, [256, 256, 1024])\n    X = residual_block(X, 3, [256, 256, 1024])\n\n    # Stage 5 \n    X = residual_block(X, 3, [512, 512, 2048], reduce=True, stride=2)\n    X = residual_block(X, 3, [512, 512, 2048])\n    X = residual_block(X, 3, [512, 512, 2048])\n    \n      # Global Average Pooling to reduce spatial dimensions\n    X = GlobalAveragePooling2D()(X)\n    \n    # Add Dropout to prevent overfitting\n    X = Dropout(0.5)(X)\n    \n    # Fully Connected Layer for classification\n    X = Dense(classes, activation='softmax')(X)\n        \n    # Create the model\n    model = Model(inputs = X_input, outputs = X, name='Modified_ResNet50')\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T16:53:47.336812Z","iopub.execute_input":"2024-03-31T16:53:47.337747Z","iopub.status.idle":"2024-03-31T16:53:47.349914Z","shell.execute_reply.started":"2024-03-31T16:53:47.337709Z","shell.execute_reply":"2024-03-31T16:53:47.349022Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\n# CBAM Module Definition\n# import tensorflow as tf\n\n# class ChannelAttention(tf.keras.layers.Layer):\n#     def __init__(self, channel_in, reduction_ratio=16):\n#         super(ChannelAttention, self).__init__()\n#         self.shared_mlp = tf.keras.Sequential([\n#             tf.keras.layers.GlobalAveragePooling2D(),\n#             tf.keras.layers.Dense(units=channel_in // reduction_ratio, activation='relu'),\n#             tf.keras.layers.Dense(units=channel_in, activation='sigmoid')\n#         ])\n\n#     def call(self, x):\n#         avg_pool = self.avg_pool(x)\n#         max_pool = self.max_pool(x)\n#         channel_attentions = self.shared_mlp(tf.concat([avg_pool, max_pool], axis=1))\n#         channel_attentions = tf.expand_dims(tf.expand_dims(channel_attentions, axis=1), axis=1)  # Add dimensions for broadcasting\n#         return x * channel_attentions\n\n# class SpatialAttention(tf.keras.layers.Layer):\n#     def __init__(self, kernel_size=7):\n#         super(SpatialAttention, self).__init__()\n#         self.compress = tf.keras.layers.Lambda(lambda x: tf.reduce_max(x, axis=3, keepdims=True) + tf.reduce_mean(x, axis=3, keepdims=True))\n#         self.spatial_attention = tf.keras.Sequential([\n#             tf.keras.layers.Conv2D(filters=1, kernel_size=kernel_size, padding='same', use_bias=False),\n#             tf.keras.layers.BatchNormalization(),\n#             tf.keras.layers.Activation('sigmoid')\n#         ])\n\n#     def call(self, x):\n#         x_compress = self.compress(x)\n#         x_output = self.spatial_attention(x_compress)\n#         return x * x_output\n\n# class CBAM(tf.keras.Model):\n#     def __init__(self, channel_in, reduction_ratio=16, spatial=True):\n#         super(CBAM, self).__init__()\n#         self.spatial = spatial\n#         self.channel_attention = ChannelAttention(channel_in, reduction_ratio=reduction_ratio)\n#         if self.spatial:\n#             self.spatial_attention = SpatialAttention()\n\n#     def call(self, x):\n#         x_out = self.channel_attention(x)\n#         if self.spatial:\n#             x_out = self.spatial_attention(x_out)\n#         return x_out\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:37.541427Z","iopub.execute_input":"2024-03-31T11:29:37.541688Z","iopub.status.idle":"2024-03-31T11:29:37.553084Z","shell.execute_reply.started":"2024-03-31T11:29:37.541665Z","shell.execute_reply":"2024-03-31T11:29:37.552274Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n\n# class BottleNeck(tf.keras.layers.Layer):\n#     def __init__(self, in_channels, out_channels, expansion=4, stride=1, use_cbam=True):\n#         super(BottleNeck, self).__init__()\n\n#         self.use_cbam = use_cbam\n#         self.conv1 = tf.keras.layers.Conv2D(filters=out_channels, kernel_size=1, strides=stride, padding='same', use_bias=False)\n#         self.bn1 = tf.keras.layers.BatchNormalization()\n#         self.conv2 = tf.keras.layers.Conv2D(filters=out_channels, kernel_size=3, strides=1, padding='same', use_bias=False)\n#         self.bn2 = tf.keras.layers.BatchNormalization()\n#         self.conv3 = tf.keras.layers.Conv2D(filters=out_channels*expansion, kernel_size=1, strides=1, padding='same', use_bias=False)\n#         self.bn3 = tf.keras.layers.BatchNormalization()\n#         self.relu = tf.keras.layers.ReLU()\n\n#         self.identity_connection = tf.keras.Sequential()\n#         if stride != 1 or in_channels != expansion*out_channels:\n#             self.identity_connection = tf.keras.Sequential([\n#                 tf.keras.layers.Conv2D(filters=out_channels*expansion, kernel_size=1, strides=stride, padding='same', use_bias=False),\n#                 tf.keras.layers.BatchNormalization()\n#             ])\n\n#         if self.use_cbam:\n#             self.cbam = CBAM(out_channels*expansion)\n\n#     def call(self, inputs):\n#         x = self.relu(self.bn1(self.conv1(inputs)))\n#         x = self.relu(self.bn2(self.conv2(x)))\n#         x = self.bn3(self.conv3(x))\n\n#         if self.use_cbam:\n#             x = self.cbam(x)\n\n#         x += self.identity_connection(inputs)\n#         x = self.relu(x)\n\n#         return x\n\n# class ResNet50(tf.keras.Model):\n#     def __init__(self, use_cbam=True, image_depth=3, num_classes=6):\n#         super(ResNet50, self).__init__()\n\n#         self.in_channels = 64\n#         self.expansion = 4\n#         self.num_blocks = [3, 3, 3, 2]\n\n#         self.conv_block1 = tf.keras.Sequential([\n#             tf.keras.layers.Conv2D(filters=self.in_channels, kernel_size=7, strides=2, padding='same', use_bias=False),\n#             tf.keras.layers.BatchNormalization(),\n#             tf.keras.layers.ReLU(),\n#             tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n#         ])\n\n#         self.layer1 = self.make_layer(out_channels=64, num_blocks=self.num_blocks[0], stride=1, use_cbam=use_cbam)\n#         self.layer2 = self.make_layer(out_channels=128, num_blocks=self.num_blocks[1], stride=2, use_cbam=use_cbam)\n#         self.layer3 = self.make_layer(out_channels=256, num_blocks=self.num_blocks[2], stride=2, use_cbam=use_cbam)\n#         self.layer4 = self.make_layer(out_channels=512, num_blocks=self.num_blocks[3], stride=2, use_cbam=use_cbam)\n#         self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n#         self.linear = tf.keras.layers.Dense(units=num_classes)\n\n#     def make_layer(self, out_channels, num_blocks, stride, use_cbam):\n#         strides = [stride] + [1]*(num_blocks-1)\n#         layers = []\n#         for stride in strides:\n#             layers.append(BottleNeck(in_channels=self.in_channels, out_channels=out_channels, stride=stride, expansion=self.expansion, use_cbam=use_cbam))\n#             self.in_channels = out_channels * self.expansion\n#         return tf.keras.Sequential(layers)\n\n#     def call(self, inputs):\n#         x = self.conv_block1(inputs)\n#         x = self.layer1(x)\n#         x = self.layer2(x)\n#         x = self.layer3(x)\n#         x_conv = self.layer4(x)\n#         x = self.avgpool(x_conv)\n#         x = tf.keras.layers.Flatten()(x)\n#         x = self.linear(x)\n#         return x_conv, x\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:37.554343Z","iopub.execute_input":"2024-03-31T11:29:37.554678Z","iopub.status.idle":"2024-03-31T11:29:37.568446Z","shell.execute_reply.started":"2024-03-31T11:29:37.554648Z","shell.execute_reply":"2024-03-31T11:29:37.567577Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# # Define the shape of the input images and number of classes\n# input_shape = (384, 384, 3)\n# num_classes = 6\n\n# # Initialize the modified ResNet50 model with the specified parameters\n# modified_resnet50_model = Modified_ResNet50(input_shape=input_shape, classes=num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:37.569526Z","iopub.execute_input":"2024-03-31T11:29:37.569806Z","iopub.status.idle":"2024-03-31T11:29:37.582374Z","shell.execute_reply.started":"2024-03-31T11:29:37.569782Z","shell.execute_reply":"2024-03-31T11:29:37.581357Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# modified_resnet50_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:37.583571Z","iopub.execute_input":"2024-03-31T11:29:37.583883Z","iopub.status.idle":"2024-03-31T11:29:37.595086Z","shell.execute_reply.started":"2024-03-31T11:29:37.583858Z","shell.execute_reply":"2024-03-31T11:29:37.594219Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# New Method\n# import tensorflow as tf\n# from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Multiply, Conv2D, Add, Activation, BatchNormalization\n\n# class ChannelAttention(tf.keras.layers.Layer):\n#     def __init__(self, ratio=8):\n#         super(ChannelAttention, self).__init__()\n#         self.avg_pool = GlobalAveragePooling2D()\n#         self.max_pool = GlobalMaxPooling2D()\n#         self.shared_mlp = Dense(units=1, activation='relu', use_bias=True, kernel_initializer='he_normal')\n#         self.ratio = ratio\n\n#     def call(self, x):\n#         avg_pool = self.avg_pool(x)\n#         max_pool = self.max_pool(x)\n#         channel_attentions = self.shared_mlp(tf.concat([avg_pool, max_pool], axis=1))\n#         return Multiply()([x, Activation('sigmoid')(channel_attentions)])\n\n# class SpatialAttention(tf.keras.layers.Layer):\n#     def __init__(self, kernel_size=7):\n#         super(SpatialAttention, self).__init__()\n#         self.conv = Conv2D(filters=1, kernel_size=kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal')\n\n#     def call(self, x):\n#         avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True)\n#         max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n#         combined = tf.concat([avg_pool, max_pool], axis=-1)\n#         return Multiply()([x, self.conv(combined)])\n\n# class CBAM(tf.keras.layers.Layer):\n#     def __init__(self, ratio=8, kernel_size=7):\n#         super(CBAM, self).__init__()\n#         self.channel_attention = ChannelAttention(ratio)\n#         self.spatial_attention = SpatialAttention(kernel_size)\n\n#     def call(self, x):\n#         x_out = self.channel_attention(x)\n#         return self.spatial_attention(x_out)\n\n# def residual_block(input_tensor, filters, kernel_size=3, stride=1, use_cbam=True):\n#     x = Conv2D(filters=filters, kernel_size=kernel_size, strides=stride, padding='same', kernel_initializer='he_normal')(input_tensor)\n#     x = BatchNormalization()(x)\n#     x = Activation('relu')(x)\n\n#     x = Conv2D(filters=filters, kernel_size=kernel_size, strides=1, padding='same', kernel_initializer='he_normal')(x)\n#     x = BatchNormalization()(x)\n\n#     if use_cbam:\n#         x = CBAM()(x)\n\n#     shortcut = input_tensor\n#     if stride != 1 or input_tensor.shape[-1] != filters:\n#         shortcut = Conv2D(filters=filters, kernel_size=1, strides=stride, padding='same', kernel_initializer='he_normal')(input_tensor)\n#         shortcut = BatchNormalization()(shortcut)\n\n#     x = Add()([x, shortcut])\n#     x = Activation('relu')(x)\n#     return x\n\n# def Modified_ResNet50(input_shape=(224, 224, 3), classes=1000):\n#     input_data = tf.keras.Input(shape=input_shape)\n#     X = Conv2D(64, (7, 7), strides=(2, 2), padding='same')(input_data)\n#     X = BatchNormalization()(X)\n#     X = Activation('relu')(X)\n#     X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n#     X = CBAM()(X)\n#     X = residual_block(X, 64, use_cbam=True)\n#     X = residual_block(X, 64, use_cbam=True)\n#     X = residual_block(X, 64, use_cbam=True)\n\n#     X = residual_block(X, 128, stride=2, use_cbam=True)\n#     X = residual_block(X, 128, use_cbam=True)\n#     X = residual_block(X, 128, use_cbam=True)\n#     X = residual_block(X, 128, use_cbam=True)\n\n#     X = residual_block(X, 256, stride=2, use_cbam=True)\n#     X = residual_block(X, 256, use_cbam=True)\n#     X = residual_block(X, 256, use_cbam=True)\n#     X = residual_block(X, 256, use_cbam=True)\n#     X = residual_block(X, 256, use_cbam=True)\n#     X = residual_block(X, 256, use_cbam=True)\n\n#     X = residual_block(X, 512, stride=2, use_cbam=True)\n#     X = residual_block(X, 512, use_cbam=True)\n#     X = residual_block(X, 512, use_cbam=True)\n\n#     X = GlobalAveragePooling2D()(X)\n#     output = Dense(classes, activation='softmax')(X)\n#     model = tf.keras.Model(inputs=input_data, outputs=output)\n#     return model\n\n# # Create the model\n# input_shape = (384,384, 3)\n# num_classes = 6\n# modified_resnet50_model = Modified_ResNet50(input_shape=input_shape, classes=num_classes)\n\n# Print model summary\n# modified_resnet50_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:34:01.185184Z","iopub.execute_input":"2024-03-31T11:34:01.186078Z","iopub.status.idle":"2024-03-31T11:34:03.664539Z","shell.execute_reply.started":"2024-03-31T11:34:01.186048Z","shell.execute_reply":"2024-03-31T11:34:03.663674Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#  ReduceLROnPlateau \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=0.00001)\n\n#  EarlyStopping \nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:51:30.703295Z","iopub.execute_input":"2024-03-31T15:51:30.703910Z","iopub.status.idle":"2024-03-31T15:51:30.708656Z","shell.execute_reply.started":"2024-03-31T15:51:30.703877Z","shell.execute_reply":"2024-03-31T15:51:30.707589Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class_labels = train_df['label'].unique()\nclass_labels","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:51:40.409632Z","iopub.execute_input":"2024-03-31T15:51:40.410264Z","iopub.status.idle":"2024-03-31T15:51:40.419498Z","shell.execute_reply.started":"2024-03-31T15:51:40.410234Z","shell.execute_reply":"2024-03-31T15:51:40.418629Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array(['trash', 'glass', 'paper', 'metal', 'plastic', 'cardboard'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"train_generator.class_indices","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:51:44.948598Z","iopub.execute_input":"2024-03-31T15:51:44.949324Z","iopub.status.idle":"2024-03-31T15:51:44.955724Z","shell.execute_reply.started":"2024-03-31T15:51:44.949290Z","shell.execute_reply":"2024-03-31T15:51:44.954680Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}"},"metadata":{}}]},{"cell_type":"code","source":"weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=train_df['label'])\nweights","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:51:48.969084Z","iopub.execute_input":"2024-03-31T15:51:48.969463Z","iopub.status.idle":"2024-03-31T15:51:48.977583Z","shell.execute_reply.started":"2024-03-31T15:51:48.969431Z","shell.execute_reply":"2024-03-31T15:51:48.976651Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([3.06212121, 0.83998337, 0.70912281, 1.02693089, 0.87489177,\n       1.04606625])"},"metadata":{}}]},{"cell_type":"code","source":"class_weights = dict(zip(train_generator.class_indices.values(),weights))\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2024-03-31T15:51:52.882801Z","iopub.execute_input":"2024-03-31T15:51:52.883684Z","iopub.status.idle":"2024-03-31T15:51:52.889544Z","shell.execute_reply.started":"2024-03-31T15:51:52.883648Z","shell.execute_reply":"2024-03-31T15:51:52.888686Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{0: 3.062121212121212,\n 1: 0.8399833748960931,\n 2: 0.7091228070175438,\n 3: 1.026930894308943,\n 4: 0.8748917748917749,\n 5: 1.0460662525879918}"},"metadata":{}}]},{"cell_type":"code","source":"# input_shape = (384, 384, 3)\n# num_classes = 6\n\n# # Initialize the modified ResNet50 model with the specified parameters\n# resnet50_model = ResNet50(input_shape,num_classes)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-31T14:18:10.807974Z","iopub.execute_input":"2024-03-31T14:18:10.808844Z","iopub.status.idle":"2024-03-31T14:18:10.812632Z","shell.execute_reply.started":"2024-03-31T14:18:10.808806Z","shell.execute_reply":"2024-03-31T14:18:10.811626Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# resnet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:40.813222Z","iopub.execute_input":"2024-03-31T11:29:40.813499Z","iopub.status.idle":"2024-03-31T11:29:40.821606Z","shell.execute_reply.started":"2024-03-31T11:29:40.813474Z","shell.execute_reply":"2024-03-31T11:29:40.820725Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# num_epochs = 10\n\n# # Train the model\n# history = resnet50_model.fit(train_generator, \n#                                       steps_per_epoch=len(train_generator), \n#                                       epochs=num_epochs, \n#                                       validation_data=val_generator, \n#                                       validation_steps=len(val_generator),\n#                                       class_weight=class_weights,\n#                                       callbacks=[reduce_lr, early_stopping])","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:29:40.822755Z","iopub.execute_input":"2024-03-31T11:29:40.823134Z","iopub.status.idle":"2024-03-31T11:29:40.831735Z","shell.execute_reply.started":"2024-03-31T11:29:40.823109Z","shell.execute_reply":"2024-03-31T11:29:40.830953Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Define the shape of the input images and number of classes\ninput_shape = (384, 384, 3)\nnum_classes = 6\n\n# Initialize the modified ResNet50 model with the specified parameters\nmodified_resnet50_model = Modified_ResNet50(input_shape=input_shape, classes=num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T16:53:56.976318Z","iopub.execute_input":"2024-03-31T16:53:56.976654Z","iopub.status.idle":"2024-03-31T16:53:59.478026Z","shell.execute_reply.started":"2024-03-31T16:53:56.976629Z","shell.execute_reply":"2024-03-31T16:53:59.477124Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"modified_resnet50_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-31T16:24:18.525487Z","iopub.execute_input":"2024-03-31T16:24:18.526324Z","iopub.status.idle":"2024-03-31T16:24:18.561786Z","shell.execute_reply.started":"2024-03-31T16:24:18.526290Z","shell.execute_reply":"2024-03-31T16:24:18.560561Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodified_resnet50_model\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n","\u001b[0;31mNameError\u001b[0m: name 'modified_resnet50_model' is not defined"],"ename":"NameError","evalue":"name 'modified_resnet50_model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"modified_resnet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:35:54.879004Z","iopub.execute_input":"2024-03-31T14:35:54.879720Z","iopub.status.idle":"2024-03-31T14:35:54.894430Z","shell.execute_reply.started":"2024-03-31T14:35:54.879693Z","shell.execute_reply":"2024-03-31T14:35:54.893525Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 100\n\n# Train the model\nhistory = modified_resnet50_model.fit(train_generator, \n                                      steps_per_epoch=len(train_generator), \n                                      epochs=num_epochs, \n                                      validation_data=val_generator, \n                                      validation_steps=len(val_generator),\n                                      class_weight=class_weights,\n                                      callbacks=[reduce_lr, early_stopping])","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:38:03.332019Z","iopub.execute_input":"2024-03-31T13:38:03.332372Z","iopub.status.idle":"2024-03-31T14:11:45.450368Z","shell.execute_reply.started":"2024-03-31T13:38:03.332348Z","shell.execute_reply":"2024-03-31T14:11:45.449242Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1711892358.783512     165 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 2s/step - accuracy: 0.3712 - loss: 1.7516 - val_accuracy: 0.1917 - val_loss: 2.2906 - learning_rate: 0.0010\nEpoch 2/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 3/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.4584 - loss: 1.4040 - val_accuracy: 0.1917 - val_loss: 1.8676 - learning_rate: 0.0010\nEpoch 4/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 5/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.5119 - loss: 1.2612 - val_accuracy: 0.2095 - val_loss: 2.1114 - learning_rate: 0.0010\nEpoch 6/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 7/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.5120 - loss: 1.2791 - val_accuracy: 0.1976 - val_loss: 2.1566 - learning_rate: 0.0010\nEpoch 8/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 9/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.5174 - loss: 1.2158 - val_accuracy: 0.1957 - val_loss: 2.5555 - learning_rate: 0.0010\nEpoch 10/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 11/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.5961 - loss: 1.1180 - val_accuracy: 0.2826 - val_loss: 2.0310 - learning_rate: 0.0010\nEpoch 12/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 13/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.5554 - loss: 1.1616 - val_accuracy: 0.3775 - val_loss: 2.0349 - learning_rate: 0.0010\nEpoch 14/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 15/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.5777 - loss: 1.0548 - val_accuracy: 0.3142 - val_loss: 3.9974 - learning_rate: 0.0010\nEpoch 16/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 0.0010\nEpoch 17/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.6014 - loss: 1.0726 - val_accuracy: 0.2154 - val_loss: 3.3045 - learning_rate: 0.0010\nEpoch 18/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 19/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.6506 - loss: 0.9234 - val_accuracy: 0.3874 - val_loss: 1.8704 - learning_rate: 5.0000e-04\nEpoch 20/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 21/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.6773 - loss: 0.8340 - val_accuracy: 0.4328 - val_loss: 2.4874 - learning_rate: 5.0000e-04\nEpoch 22/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 23/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.6996 - loss: 0.7883 - val_accuracy: 0.2925 - val_loss: 3.2627 - learning_rate: 5.0000e-04\nEpoch 24/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 25/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.7214 - loss: 0.7377 - val_accuracy: 0.3557 - val_loss: 2.6312 - learning_rate: 5.0000e-04\nEpoch 26/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 27/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.7147 - loss: 0.7241 - val_accuracy: 0.4012 - val_loss: 2.0362 - learning_rate: 5.0000e-04\nEpoch 28/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 29/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.7356 - loss: 0.7176 - val_accuracy: 0.2826 - val_loss: 3.3195 - learning_rate: 5.0000e-04\nEpoch 30/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 31/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.7496 - loss: 0.6601 - val_accuracy: 0.3854 - val_loss: 3.6412 - learning_rate: 5.0000e-04\nEpoch 32/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\nEpoch 33/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.7614 - loss: 0.5893 - val_accuracy: 0.4545 - val_loss: 1.8696 - learning_rate: 2.5000e-04\nEpoch 34/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 35/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 0.7797 - loss: 0.5603 - val_accuracy: 0.6957 - val_loss: 0.8630 - learning_rate: 2.5000e-04\nEpoch 36/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 37/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 0.8001 - loss: 0.4901 - val_accuracy: 0.5909 - val_loss: 1.5548 - learning_rate: 2.5000e-04\nEpoch 38/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 39/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.8116 - loss: 0.4794 - val_accuracy: 0.5237 - val_loss: 1.4444 - learning_rate: 2.5000e-04\nEpoch 40/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 41/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - accuracy: 0.8156 - loss: 0.4835 - val_accuracy: 0.5435 - val_loss: 1.5502 - learning_rate: 2.5000e-04\nEpoch 42/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 43/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.8250 - loss: 0.4467 - val_accuracy: 0.3992 - val_loss: 3.0785 - learning_rate: 2.5000e-04\nEpoch 44/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 45/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.8226 - loss: 0.4469 - val_accuracy: 0.2787 - val_loss: 6.1364 - learning_rate: 2.5000e-04\nEpoch 46/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\nEpoch 47/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.8416 - loss: 0.4201 - val_accuracy: 0.4249 - val_loss: 2.6434 - learning_rate: 2.5000e-04\nEpoch 48/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 1.2500e-04\nEpoch 49/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.8502 - loss: 0.3816 - val_accuracy: 0.8083 - val_loss: 0.5587 - learning_rate: 1.2500e-04\nEpoch 50/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 1.2500e-04\nEpoch 51/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.8676 - loss: 0.3420 - val_accuracy: 0.8004 - val_loss: 0.5829 - learning_rate: 1.2500e-04\nEpoch 52/100\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - learning_rate: 1.2500e-04\nEpoch 52: early stopping\nRestoring model weights from the end of the best epoch: 2.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}